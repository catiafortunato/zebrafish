{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model we are using convLSTM to predict the next frame of a video based on past video sequences. We use convLSTM, which is a LSTM layer but the input transformation and gate operations are replaced by convolutional operations, that allow to capture spatio-temporal dependencies. The input video will be the downsampled version of the original video, with size 8x8 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=os.listdir('/home/catia/Desktop/zebrafish/data')\n",
    "dir.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum=1000\n",
    "maximum=0\n",
    "selected_slice=23\n",
    "j=0\n",
    "for i in range(selected_slice,len(dir),100):\n",
    "    im=h5py.File('data/'+dir[i],'r+')\n",
    "    img=list(im['d1']) \n",
    "    #print (dir[i])\n",
    "    new_min=np.min(img)\n",
    "    new_max=np.max(img)\n",
    "    if new_min < minimum:\n",
    "        minimum=new_min\n",
    "        #print (minimum)\n",
    "    if new_max > maximum:\n",
    "        maximum=new_max\n",
    "        #print (maximum)\n",
    "    j=j+1\n",
    "    \n",
    "    \n",
    "num_img=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.zeros((num_img,400,400))\n",
    "j=0\n",
    "\n",
    "# Data starts by having values between minimum and maximum\n",
    "for i in range (selected_slice,len(dir),100):\n",
    "    im=h5py.File('data/'+dir[i],'r+')\n",
    "    img=list(im['d1']) \n",
    "    img=img-minimum # So the data is between 0 and maximum\n",
    "    img=img/(0.5*(maximum-minimum)) # So the data is between 0 and 2\n",
    "    img=img-1 # So the data is between -1 and 1\n",
    "    data[j,:,:]=img # Store the image into the variable data\n",
    "    j=j+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_data=np.zeros((num_img,8,8))\n",
    "for i in range(0,len(down_data)):\n",
    "    img=data[i,:,:]\n",
    "    dimg=np.zeros((8,8))\n",
    "    for j in range (0,8):\n",
    "        for k in range (0,8):\n",
    "            dimg[j,k]=np.mean(img[j*50:j*50+50,k*50:k*50+50])\n",
    "    down_data[i,:,:]=dimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8839, 8, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "down_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network can't receive the entire training data set at once, so we have to reshape the data to (sequence,timesteps,row,col,1), `sequence` is the number of sequence examples, `timesteps` is how far into the past we are looking in each example, and `row`,`col`and `1` is the shape of the images since we only have one channel (gray scale images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data=down_data[0:7000,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 8, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "past=20 #how far into the past we are looking\n",
    "input_data=np.zeros((len(x_train_data)-past,past,x_train_data.shape[1],x_train_data.shape[2],1))\n",
    "\n",
    "for i in range(past,len(x_train_data)):\n",
    "    input_data[i-past,:,:,:,0]=x_train_data[i-past:i,:,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8, 8)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_data[20-20:20,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6980, 20, 8, 8, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8, 8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[4,:,:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_data=down_data[past+1:7001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6980, 20, 8, 8, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_data=y_train_data.reshape((6980, 1, 8, 8, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6980, 1, 8, 8, 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Sequential()\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   input_shape=(None, 8, 8, 1),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "seq.compile(loss='binary_crossentropy', optimizer='adadelta',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_25 (ConvLSTM2D) (None, None, 8, 8, 40)    59200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, None, 8, 8, 40)    160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_26 (ConvLSTM2D) (None, None, 8, 8, 40)    115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, None, 8, 8, 40)    160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_27 (ConvLSTM2D) (None, None, 8, 8, 40)    115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, None, 8, 8, 40)    160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_28 (ConvLSTM2D) (None, None, 8, 8, 40)    115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, None, 8, 8, 40)    160       \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, None, 8, 8, 1)     1081      \n",
      "=================================================================\n",
      "Total params: 407,001\n",
      "Trainable params: 406,681\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6631 samples, validate on 349 samples\n",
      "Epoch 1/30\n",
      "6631/6631 [==============================] - 1035s 156ms/step - loss: -12.4764 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "6631/6631 [==============================] - 1053s 159ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "6631/6631 [==============================] - 1026s 155ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "6631/6631 [==============================] - 1032s 156ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "6631/6631 [==============================] - 1012s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "6631/6631 [==============================] - 1023s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "6631/6631 [==============================] - 1014s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "6631/6631 [==============================] - 1020s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "6631/6631 [==============================] - 1011s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "6631/6631 [==============================] - 1024s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 11/30\n",
      "6631/6631 [==============================] - 1006s 152ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "6631/6631 [==============================] - 1019s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "6631/6631 [==============================] - 1026s 155ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "6631/6631 [==============================] - 1013s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "6631/6631 [==============================] - 1025s 155ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "6631/6631 [==============================] - 1017s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "6631/6631 [==============================] - 1017s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "6631/6631 [==============================] - 1013s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "6631/6631 [==============================] - 1027s 155ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "6631/6631 [==============================] - 1021s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "6631/6631 [==============================] - 1020s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "6631/6631 [==============================] - 1015s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "6631/6631 [==============================] - 1022s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "6631/6631 [==============================] - 1011s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "6631/6631 [==============================] - 1030s 155ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "6631/6631 [==============================] - 1018s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "6631/6631 [==============================] - 1033s 156ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "6631/6631 [==============================] - 1020s 154ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "6631/6631 [==============================] - 1026s 155ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "6631/6631 [==============================] - 1015s 153ms/step - loss: -12.8174 - acc: 0.0000e+00 - val_loss: -12.8587 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20dafca5f8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.fit(input_data, y_train_data, batch_size=10,\n",
    "        epochs=30, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6980, 8, 8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
