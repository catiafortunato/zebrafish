{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB movie review classification problem: sequence to class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(\n",
    "     num_words=max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By downloading the data, we get 250000 examples of movie reviews but each example as a differente size, so we use sequence.pad_sequences in order to obtain all examples the same size. The input to the network has the size (sequences, timesteps,features), where ``sequences`` is the number os examples, `timesteps` is the number of measurments obtained in each sequence and `features` is the number os variables in each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 14s 685us/step - loss: 0.5870 - acc: 0.6724 - val_loss: 0.5169 - val_acc: 0.7508\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 14s 708us/step - loss: 0.3689 - acc: 0.8488 - val_loss: 0.4033 - val_acc: 0.8228\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 14s 696us/step - loss: 0.2933 - acc: 0.8868 - val_loss: 0.3214 - val_acc: 0.8758\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 14s 703us/step - loss: 0.2146 - acc: 0.9231 - val_loss: 0.3450 - val_acc: 0.8694\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 14s 693us/step - loss: 0.1490 - acc: 0.9470 - val_loss: 0.3580 - val_acc: 0.8704\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 14s 679us/step - loss: 0.1005 - acc: 0.9665 - val_loss: 0.3881 - val_acc: 0.8686\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 14s 684us/step - loss: 0.0637 - acc: 0.9804 - val_loss: 0.5283 - val_acc: 0.8312\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 13s 672us/step - loss: 0.0396 - acc: 0.9884 - val_loss: 0.6128 - val_acc: 0.8110\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 13s 668us/step - loss: 0.0272 - acc: 0.9915 - val_loss: 0.7695 - val_acc: 0.7484\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 13s 666us/step - loss: 0.0202 - acc: 0.9942 - val_loss: 0.6020 - val_acc: 0.8260\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first naive approach to this dataset got you to a test accuracy of 88%. Unfortunately, this small recurrent network doesn’t perform well compared to this baseline (only 85% validation accuracy). Part of the problem is that your inputs only consider the first 500 words, rather than full sequences—hence, the RNN has access to less information than the earlier baseline model. The remainder of the problem is that SimpleRNN isn’t good at processing long sequences, such as text. Other types of recurrent layers perform much better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, in this example we are only trying to predict the class of each movie review, we are not trying to predict any value into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
